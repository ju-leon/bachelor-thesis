%% LaTeX2e class for student theses
%% sections/content.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.5, 2020-06-26

\chapter{Introduction}
\label{ch:Introduction}

%% -------------------
%% | Example content |
%% -------------------

Catalysts in combination with chemical reactions are used to speed up a reaction by lowering its activation energy. 
Catalysts have an activation energy them self that needs to be overcome in oder to start the reaction, the so called reaction barrier.

The reaction barrier depends on the structure of the catalyst, and varies greatly between different catalyst molecules.
Intuitively no rule for the activation barrier can be found.
Seemingly small changes in the catalysts shape can have a large influence on the catalysts activation barrier Figure~\ref{fig:struct-diff}.
A rule to guess the activation barrier from the molecules structure can not easily be found.  
Knowing how to change catalyst molecules in order to lower their activation barrier is a difficult challenge, even to humans. %TODO: Elaborate
While the activation barrier can be computed, this process is highly complex and takes a lot of computing power.
This means computing the activation barrier for large datasets of catalysts is currently not feasible.
\\
In this Bachelor's thesis, different approaches to use machine learning to compute the activation barrier of catalyst molecules are explored.
Multiple methods of encoding catalyst molecules into machine-understandable formats are proposed.
Using artificial neural networks, the activation barrier is predicted from the catalysts shape.

After being able to predict the activation barrier with high accuracy, different techniques to explain the 
outputs of the neural network are used.
This helps to better understand booth the network that is usually regarded as a black box, and the feature space.
The explainers used here are simple gradient based methods, and SHAP explainers \cite{NIPS2017_7062}.
This allows for intuition on which parts of the catalyst molecule contribute to the prediction of the activation barrier.
This intuition may be helpful in further chemical analysis of the metal catalyst and can help to give the 
chemist an idea on how an element needs to be changed in oder to lower it's activation barrier.
\\

\begin{figure}
  \centering
  \includegraphics[width=0.8\textwidth]{figures/introduction/elems_intro.png}
  \caption{Two elements with seemingly very similar chemical structures. The chemical structure 1 and it's corresponding 3D structure 2 have an activation barrier of $3.6 kcal/mol$.
  Molecule 3 and it's corresponding 3D structure 4 have an activation barrier of $18.0 kcal/mol$.
  The only difference in the 3D structure of the 2 is the fluorine arm is being replaced by a nitrogen arm.
  The difference is significant considering the distribution of the activation barriers in \ref{fig:barriers}.  }
  \label{fig:struct-diff}
\end{figure}


The metal catalysts used used here are constructed by combining different ligands around a central Iridium atom.
As illustrated in Figure~\ref{fig:chemspace}, the central atom has multiple locations that allow different types of ligands to be attached.  
This allows for quick generation of thousands of different catalyst molecules.
By adding more ligands, the dataset can be increased in size with little effort.
For all catalysts in the dataset the activation barrier, along with other properties, is then computed.
The structure, both chemical and spacial, and the activation barrier is know for every example in the dataset. 
\\
As with many machine learning tasks, feature selection is one of the big challenges in this project.
While there are many universal techniques to extract features from chemical structures, they all have their drawbacks.

In previous works, multiple different techniques are proposed to extract features from a catalyst molecule \cite{friederich_dos}.
These techniques are based on the chemical structure of a molecule, but do not take into account its 3D spacial structure.
This withholds a lot of information to the neural network that might help it to more accurately predict the activation barrier.
The feature extracting methods developed in this bachelor thesis will rely heavily on the 3D structure of the molecule.
The idea being that the 3D structure plays an important role in the activation barrier, and encoding the 3D structure will enhance regression accuracy.
Additionally encoding the 3D structure will allow to learn about the space surrounding the central 
Iridium atom to understand the importance of location of the atoms in our molecule.

In order to use feature explainers on the neural network, a mapping back from feature space and chemical space needs to be possible.
There currently is no widespread way to encode catalyst molecules that also allows for 
reconstruction of the 3D space and by that an intuition on how the molecule needs to be changed in order ot lower the activation barrier.
Since most chemical feature generators do not specifically focus on catalyst molecules,
but rather provide features for a wide variety of molecules, they generally tend to generate a fully rotational invariant output.
This full rotational invariance means that information about the location of single atoms is often lost or only implicitly encoded in the features.
A reconstruction of the molecule only from the features is therefore not possible.
The descriptors proposed here allow make use of a catalysts special structure.
This allows for partial reconstruction of the 3D shape of the molecule and could in the future be used to also encode 
other properties of the molecule.
\\

The seemingly arbitrary distribution of activation barriers among catalyst molecules  was reason to use neural networks for prediction from the generated features.
Neural networks have become the go-to method for high dimensional regression and classification for their ability to adapt well to complex data.
\\
A limitation of neural networks however is their fixed-size input space.
Therefor a representation of the catalyst needs to be found that encodes molecules into a fixed-size set of features.
These features will then be fed into our neural network for training. 
\\
3D structural encoding comes with its own set of challenges. 
One more general being neural networks sensitivity to rotation and positioning.
Since a molecules activation barrier does not change depending on its rotation or location in space, 
information about rotation and translation should ideally not be part of the molecules features.
\\
In the case of the metal catalysts, achieving translational invariance is trivial.
Since every catalyst is constructed around exactly one metal atom, the molecule can be centered around this metal atom.

For rotational invariance the problem is more complex.
Every catalyst has a reaction pocket attached to it's cental atom.
This reaction pocket has a fixed position.
With the vector from the center of the Iridium atom to the center of the reaction pocket, two more degrees of freedom can be removed.
For the last degree of freedom, rotations around this vector, there is no natural way to get rid of it.
%TODO: Figure
Here, 2 different approaches to this last degree of freedom are explored.
The first is using a descriptor that is invariant under rotations around the last degree of freedom.
This removes the need for further normalizations, since all features generated by this descriptor will be invariant.

The second is using data augmentation to teach the neural network about all possible rotations.
This means the molecule is rotated along the last remaining axis of freedom, and and multiple examples of the same molecule at different rotations are used as training examples for the neural network.
This ideally allows the network to learn the molecules structure independent of it's rotation.

After training the networks allow for a prediction of a catalysts activation barrier from it's 3D structure.
The main motivation behind this structural approach  comes from the ability to analyse the networks after training.
By analyzing on which features the network is basing it's prediction, and translating these features back into 
3D space, we can get an intuition on which parts of the molecule influence it's activation barrier.
Since SNAP encodes different species of atoms separately, the influence of different atom species 
can also be observed.

\section{Dataset}

The dataset used for training and testing contains a total of 1947 samples.
Each sample describes the 3D structure of one catalyst molecule, 
holding the position for every atom in that molecule.
Additional information about the molecule was precomputed, among it the reaction barrier focused on in this work Figure~\ref{fig:barriers}.
The number of atoms forming one molecule varies between molecules.
What's consistent is the central iridium atom and the reaction pocked, a single hydrogen atom, attached to the center.
The global position and rotation of the molecule within the dataset is seemingly arbitrary.
Global rotation and location of the element does not influence it's activation barrier.
However the local positions of atoms in the molecule plays and important role in the activation barrier.

\begin{figure}
  \centering
  \includegraphics[width=7cm]{figures/introduction/barrier.png}
  \caption{Distribution of the elements activation barrier. The elements have a mean of $11.970 kcal/mol$ and a standart deviation of $4.33 kcal/mol$.}
  \label{fig:barriers}
\end{figure}

The dataset was constructed from a set of ligands.
These ligands are placed around the iridium core in a specific order.
There is a total of 3 ligand groups, with every group being able to attach in a different way to the metal center.
By combining different ligands from these groups, catalysts are created Figure~\ref{fig:chemspace}.
These chemical structures are known as Vaska's complex.

Due to this combinatorial approach the dataset could later be increased with relatively low effort.
The biggest hurdle to increasing the dataset size is computing the activation barrier.
This computation is highly complex and therefore not feasible for large datasets.
However, generating a larger dataset without computing the activation barrier could still be helpful when using transfer learning approaches to fine tune the network.
This idea is further discussed in \ref{ch:Conclusion}.

\begin{figure}
  \centering
  \includegraphics[width=10cm]{figures/introduction/chem-space.png}
  \caption{Ligands defining the chemical space associated with Vaska's complex. Reprinted from \cite{friederich_dos}.}
  \label{fig:chemspace}
\end{figure}


\section{Previous research}

In previous approaches different machine learning methods were used to predict the activation barrier of the elements in this dataset.
The features extracted from the elements focused on their atomic properties.
However, the features extracted from the molecule did not take into account the spacial structure of the element.
The elements were instead encoded by creating a graph from the chemical structure.
The elements are then grouped by their distance from the metal center.
For each group of elements, different features are computed as sum of pairwise products/differences of their atomic properties(such as electronegativity, atomic number, identity, topology and size) \cite{friederich_dos}.
Note that these features do not contain any information about the 3D location of the atoms.

Using these autocorrelation features, a neural network and gaussian processes and other forms of regression were used to predict the activation barrier.
Neural networks and gaussian processes both were able to predict the activation barrier within an error of $<1 kcal/mol$ for a test split of $20\%$. %TODO: Evtl genauer?

Other than the obvious disadvantage of not encoding location information, another disadvantage is the lack of interpretability of results.
While these features succeed at predicting reaction barriers, gaining 
information about what part of the molecule is contributing to the prediction is limited or not possible.

In a yet unpublished approach, the autocorrelation features were replaced by a graph structure of the molecule.
On this graph structure, a graph convolutional neural network was trained.
When training on a large fraction of the dataset this network reached accuracies 
beyond what could be achieved using autocorrelation features.

The feature extractors introduced in this thesis aim to solve this problem by extracting features that allow for a
partial reconstruction of the chemical space.
This helps to better understand the origin of a prediction in a spacial sense.

Feature extraction is a common problem for machine learning methods in chemical spaces.
Multiple approaches have been proposed for general molecule encoding, 
ranging from encoding properties of molecules, such as the Coulomb Matrix encoder encoding electrostatic interaction of atoms \cite{PhysRevLett.108.058301}
to encoding 3D structures of atomic environments \cite{Bart_k_2013}.

3D structural encoders usually create a fully invariant features description.
In the case of SOAP proposed by \citeauthor{Bart_k_2013}, this is achieved by encoding information about the interaction of 
different species rather than encoding the 3D space itself.
Fully invariant 3D descriptors have the advantage of being universally applicable for any kind of molecule, since they have no requirements to the element being encoded.
The disadvantages are that, once the features are generated, a transformation back to 3D space is usually not possible.
In many applications, this is not a problem since the generated features are used only for prediction of an elements properties.
In our case, the features however should later allow us to interpret the 3D space surrounding the molecule and ideally give an idea on how the molecule can be changed to alter it's properties.

The idea of using a catalysts special structure for this task, and therefor removing some axes of freedom from the feature space, seems to be an unexplored approach.

Since the structure of the catalyst still leaves 1 degree of freedom, the prediction needs to be partly rotationally invariant.
This is a common problem for neural networks even outside of chemistry.
Generally the approaches to solving these problems can be divided into 2 different groups.

The first being feature engineering to generate features from the data that is fully invariant of rotation.
In point clouds approaches include representating the data as angles and distances rather than the points itself \cite{8886052,weiler20183D}.
Since these features do not change depending on the rotation and translation of the points, the network does no need to 
abstract away the rotational information of the data.
While this method guarantees full rotational invariance, it requires extensive feature engineering.
Additionally, it may increase complexity of the features and in many cases 
goes hand in hand with the loss of some data from the original dataset. %TODO: Remove?
%The EFD feature generator proposed here implements a fully rotationally invariant description by using 
%descriptor that can be normalized for rotation.

A second approach to rotational invariance is data augmentation.
In image recognition data augmentation has already become the go-to method.
Data augmentation removes the need for extensive feature engineering.
Instead, the training data is augmented along all axes of freedom.
In the case of image recognition, this means rotating, scaling, and in some cases deforming the input images.
By that, the dataset will be filled with more examples for every datapoint, and ideally the model is able to 
abstract away these features. %TODO: Citation neededs
%The SNAP feature generator proposed here produces a partially rotationally invariant output that needs to be augmented along one axis.
%All the augmentation steps are then fed to the networks in order to teach the network to abstract away the different rotations of the catalyst.

\section{Objectives}

As with many machine learning tasks, using the raw data as input to machine learning models is not feasible.
Features are therefore generated from the raw data that cen be fed be used to learn from.
The typical workflow for machine learning in chemical fields reflects this approach.

\begin{figure}
  \centering
  \includegraphics[width=12cm]{figures/introduction/chem-descriptor.jpg}
  \caption{Visualzation of feature generation and prediction from the generated features. 
  A fixed size descriptor is generated from the structure. Using machine learning techniques, 
  predictions are made from these features.
  The feature generators propsed here should allow a partial inversion of this process to allow for interpretability of the results.
  Reprinted from \cite{dscribe}.}
  \label{fig:chemspace}
\end{figure}

This work can be grouped into 2 main objectives. 
The first is to find a feature extractor that generates features from a catalyst molecule that ideally rotationally invariant.
The second objective is to train a neural network on these features that predicts the activation barrier.
The goal was to achieve accuracy similar or higher to what \citeauthor{friederich_dos} achieved in \cite{friederich_dos}.
In a final step, using explainers to explain the origins of the networks prediction, an intuition on how a catalyst has 
to be adapted in order to change it's activation barrier is given.

\subsection{Feature generation}

The features should allow a regressor to make predictions from the euclidean space surrounding the central Iridium atom.

Therefor features have to be of fixed length for all elements in the dataset.
The amount of atoms in one element cannot influence the number of features.

Ideally the number of features is as low as possible, helping the model to identify the relevant features better.
The number of features cannot be too small as it should still contain all relevant information.
For interpretability of the results, the features should allow for a partial or full reconstruction of the euclidean space they are encoding.
This means, given the features, it should be possible to approximate the general shape of the molecule.

To allow for computational exploration of the encoded space, the features should be continuos.
Small changes in the density space should result in small changes in the 
feature space. 
This means hashing from chemical space to feature space is not viable.

The global location and rotation of the element should not influence the features or have only limited influence on the features.

Two different approaches to feature generation are prosed.
The first is a fully rotationally invariant output using a rotationally invariant contour description.
While the output of this descriptor is fully invariant, it suffers from sampling issues due to the nature of it's encoding.

The second approach to feature generation was using a combination of basis functions to fully construct a local SE(3) environment around the central atom.
Using a set of coefficients, the density space surrounding the central atom can be approximated.

\subsection{Regression}

The second step is to predict the activation barrier from these features using an artificial neural network.
The goal was to find a network able to to predict the activation barrier with accuracy similar or better to the machine learning methods proposed by \citeauthor{friederich_dos}.
The networks proposed here achieve higher accuracy's than all the best regression methods proposed in \cite{friederich_dos}.

Especially the regression methods performed on SNAP features achieve accuracies significantly higher than regression methods on graph convolutions.

\subsection{Explaining the feature space}
Since the features should be able to allow a inversion back to the 3D space, it's possible to approximate which areas in 3D space are responsible for the prediction.
Using neural networks explainers, in  a last step the regions in 3D space that influence the regression will be analyzed.
This gives an idea on how different atoms in the dataset influence the prediction.
Looking at the gradient of the input with respect to the activation barrier, an intuition on how the catalyst molecule needs to be 
adapted in order to decrease the activation barrier can be given.

This analysis is performed on SNAP features. 
Due to the low resolution of SNAP features, the information that can be gained is limited.