%% LaTeX2e class for student theses
%% sections/conclusion.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.5, 2020-06-26

\chapter{Conclusion}
\label{ch:Conclusion}
In chapter \ref{ch:features} 2 different approaches at encoding a molecule into computer readable features are introduced.
On the features produced by the descriptors, regression using neural networks is performed.

While both methods allow for predictions with high accuracy, the results on SNAP features are more promising.
Despite the output not being fully rotational invariant, the network seems to still be able to abstract the rotation away with high accuracy.

One of the key differences between the features generated by the descriptors is their information about 
different species.
While the LEFD descriptor only describes the overall shape of the element, it does not encode any information about the 
type of atom in each layer.
The species included in a layer are only implicitly encoded by their unique Van-der-Waals radius, but no explicit information about the 
atom itself is encoded.
In contrary, the SNAP descriptor encodes the density for every species separately.

Booth the LEFD and SNAP features allow the networks to achieve relatively high accuracies.
This leads to the conclusion that a the shape of a catalyst plays a significant role in it's activation barrier.
Even without any further knowledge about the underlying chemistry, the regression on LEFD features achieved higher accuracies than
the best neural networks on autocorrelation features that encode chemical properties \cite{friederich_dos}.
This interpretation comes with a couple of asterics.
Since the dataset is relatively small and with relatively small variation, it is possible that there is a rule
to the activation barriers of the catalysts in the dataset that was encoded in the LEFD features and in the autocorrelation features. 
If that was the case, the network might have in fact learned that rule, rather than learned to 
guess the activation barrier from the features.
This theory is supported by the fact that autocorrelation features and LEFD features achived very similar regression accuracies.
The purpose of a test dataset is to prevent overfitting issues like this.
All the networks performed similar on previously unseen test data as on the data used during training.
However, due to the comminatory nature of the dataset, there might be correlation 
between the selection of ligands and the activation barrier.
So it is possible that the network learned the ligand combination, rather to make actual predictions from the chemical structures.
The dataset is currently being extended with more ligands.
The new ligands will allow to test the network with further examples of higher variation.
Once the extended dataset becomes available, it will be possible to make a more definite statement about the networks ability to 
perform regression on the shape of chemical elements.
If the network will achieve similar accuracies on the extended dataset, this allows for further interpretations in the chemical field.
Since the network makes predictions solely on the shape of the element, it indicates that there is a strong
correlation between the structural shape of a catalyst and it's activation barrier.

In contrast, SNAP features encode the density for every species separately.
This allows the neural network to learn the influence of each species separately, and extract information about the interaction of different species.
When looking at network explainers for neural networks trained on SNAP features, the network seems to
heavily depend on the densities for the different species.
Looking at structures with similar shape but a different composition of atoms, the model bases 
its decision heavily on the species that are different between the 2 elements. %TODO: Add SHAP figure
This explains the vastly higher accuracy of SNAP features compared to LEFD features.
Since the SNAP neural networks not only depend on spacial structure, but also take the interaction of different chemical 
elements into account, the hope is that the classification results will be more robust to drastic changes in the dataset.
This hypothesis will have to be verified once the extended dataset with new ligands becomes available.

Apart from the speedup that neural networks bring over computing the activation barrier the manual way,
another interesting application of the models found here is when thinking of them as a function mapping from chemical space $\mathbb{D}$ to SNAP feature space 
$\mathbb{R}^n =: \mathbb{S}$ to it's activation barrier in $\mathbb{R}^+_0$.

$$ \Psi : \mathbb{D} \to \mathbb{S} \to \mathbb{R}^+_0, e \mapsto \Psi(e) $$

The derivative of $\nabla \Psi$ is a gradient that points in  a direction in chemical space.
Since $\Psi$ is mapping from chemical space to an approximation of the activation barrier,
following the gradient will decrease the barrier of of the element.
Performing gradient descent starting at the encoding of a catalyst, an intuition on how the 
molecule has to be changed can be given.

In further works, this gradient decent approach can be implemented further.
Since gradient descent suffers from problems such as convergence into local minima, 
it needs extensive tuning in oder to properly fit the problem.

Another challenge is the mapping between the real world $\mathbb{D}$ and the features generated by SNAP $\mathbb{S}$.
If we look back, one of the main goal of SNAP was the possibility to reproduce the space from it's features.
This is important now, since after performing gradient decent, it will be possible to transform the coefficients
back into a 3d representation.
From this representation, a chemical structure can then be approximated.
The accuracy of mapping from the 3d representation to chemical space is determined by the accuracy of the 3d representation.
As shown in chapter XXX, the accuracy of representation also influence the networks prediction performance.
The quality of the gradient will also depend on the networks overall performance.
In the end, a tradeoff between network accuracy and accuracy of mapping between chemical space and 3d representation has to be made.

\section{Outlook}

The possibility of mapping both ways between the real world and SNAP features is what set the SNAP feature generator apart 
from many other chemical feature generators.
It allows for many different approaches to computational element discovery.
\\
Another area where SNAP features could be used is auto encoders or generative adversarial neural networks (GANs).
Both autoencoders and GANs are machine learning methods that can be used to produce never seen before data with similar properties
to the training data.
GANs and autoencoders have recently become very popular in image synthesis for their ability to generate synthetic images 
virtually indistinguishable from real pictures \cite{karras2019stylebased} .
Using these techniques, but applying them to SNAP features of catalysts, rather than to images, synthesis 
of catalyst molecules is thinkable.
The approach is again limited by the accuracy of mapping between chemical space and feature space.
Another factor to be considered is if SNAP features actually encode all relevant information needed.

While the accuracies achieved by the models trained on SNAP features indicate that 
the majority of information needed to predict the activation barrier is encoded into the SNAP features, this does
not imply it holds true for other properties of the molecule.
In the future, the SNAP features might be aggregated with other properties of the molecule.
Possible ways to add more information to the SNAP features is to encode the space surrounding the 
central atom in the same way it is encoded now, but instead of encoding density information encoding 
other molecular properties, such as electro negativity.

While convolution layers did not prove effective in for the current SNAP features,
when changing the feature space encoded by snap, and therefor the dimensionality of the input vector,
convolutions might become relevant again.

In conclusion the SNAP feature extractor shows that data augmentation is possible in chemical spaces, and therefore 
feature generators that allow for a bi-directional mapping between features and chemical space can achieve similar or higher
accuracies to state-of-the-art one-directional feature extractors.
This allows for the use of bi-directional feature encoders in computational exploration and generation of chemical structures.
