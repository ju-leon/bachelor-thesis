%% LaTeX2e class for student theses
%% sections/conclusion.tex
%% 
%% Karlsruhe Institute of Technology
%% Institute for Program Structures and Data Organization
%% Chair for Software Design and Quality (SDQ)
%%
%% Dr.-Ing. Erik Burger
%% burger@kit.edu
%%
%% Version 1.3.5, 2020-06-26

\chapter{Conclusion}
\label{ch:Conclusion}
In chapter \ref{ch:features} 2 different approaches at encoding a molecule into computer readable features are introduced.
On the features produced by the descriptors, regression using neural networks is performed.

While both methods allow for predictions with high accuracy, the results on SNAP features are more promising.
Despite the output not being fully rotational invariant, the network seems to still be able to abstract the rotation away with high accuracy.

One of the key differences between the features generated by the descriptors is their information about 
different species.
While the LEFD descriptor only describes the overall shape of the element, it does not encode any information about the 
type of atom in each layer.
The species included in a layer are only implicitly encoded by their unique Van-der-Waals radius, but no explicit information about the 
atom itself is encoded.
In contrary, the SNAP descriptor encodes the density for every species separately.

Booth the LEFD and SNAP features allow the networks to achieve relatively high accuracies.
This leads to the conclusion that a the shape of a catalyst plays a significant role in it's activation barrier.
Even without any further knowledge about the underlying chemistry, the regression on LEFD features achieved higher accuracies than
the best neural networks on autocorrelation features that encode chemical properties \cite{friederich_dos}.
This interpretation comes with a couple of asterics.
Since the dataset is relatively small and with relatively small variation, it is possible that there is a rule
to the activation barriers of the catalysts in the dataset that was encoded in the LEFD features and in the autocorrelation features. 
If that was the case, the network might have in fact learned that rule, rather than learned to 
guess the activation barrier from the features.
This theory is supported by the fact that autocorrelation features and LEFD features achived very similar regression accuracies.
The purpose of a test dataset is to prevent overfitting issues like this.
All the networks performed similar on previously unseen test data as on the data used during training.
However, due to the comminatory nature of the dataset, there might be correlation 
between the selection of ligands and the activation barrier.
So it is possible that the network learned the ligand combination, rather to make actual predictions from the chemical structures.
The dataset is currently being extended with more ligands.
The new ligands will allow to test the network with further examples of higher variation.
Once the extended dataset becomes available, it will be possible to make a more definite statement about the networks ability to 
perform regression on the shape of chemical elements.
If the network will achieve similar accuracies on the extended dataset, this allows for further interpretations in the chemical field.
Since the network makes predictions solely on the shape of the element, it indicates that there is a strong
correlation between the structural shape of a catalyst and it's activation barrier.

In contrast, SNAP features encode the density for every species separately.
This allows the neural network to learn the influence of each species separately, and extract information about the interaction of different species.
When looking at network explainers for neural networks trained on SNAP features, the network seems to
heavily depend on the densities for the different species.
Looking at structures with similar shape but a different composition of atoms, the model bases 
its decision heavily on the species that are different between the 2 elements. %TODO: Add SHAP figure
This explains the vastly higher accuracy of SNAP features compared to LEFD features.
Since the SNAP neural networks not only depend on spacial structure, but also take the interaction of different chemical 
elements into account, the hope is that the classification results will be more robust to drastic changes in the dataset.
This hypothesis will have to be verified once the extended dataset with new ligands becomes available.

Apart from the speedup that neural networks bring over computing the activation barrier the manual way,
another interesting application of the models found here is when thinking of them as a function mapping from chemical space $\mathbb{D}$ to SNAP feature space 
$\mathbb{R}^n =: \mathbb{S}$ to it's activation barrier in $\mathbb{R}^+_0$.

$$ \Psi : \mathbb{D} \to \mathbb{S} \to \mathbb{R}^+_0, e \mapsto \Psi(e) $$

The derivative of $\nabla \Psi$ is a gradient that points in towards the steepest ascent.
Since the gradient is the vector the size of the input space, by adding the gradient to coefficients of the element 
the coefficients of an element with similar properties but lower activation barrier will be created.
This theoretical approach however is not viable in practice.
The low density of the encoding means that the a translation back from the altered molecule will not be possible.
Additionally, the gradient often points to illegal configurations that are not possible in chemistry.
Following a simple gradient descent approach to find a model with ideal properties is therefor not viable.
Learning an intuition about the molecule in close proximity to the original coefficients however is sometimes possible.

Due to the limited resolution, looking at the gradient itself is more valuable.
When looking at the gradient, it is expected that molecules known to increase the barrier will generate a sub-zero density in the gradient.
This would indicate that removing these element will lower the barrier.
Molecules known to decrease the barrier should generate a high density, indicating adding more density in 
this region will further decrease the barrier. (?????)

Since the gradient is determined by calculating the derivative of the the network $N$ with input $c$ in respect to the output $N(c): \mathbb{R^N} \to \mathbb{R}$

$$
\nabla N: \mathbb{R}^n  \to \mathbb{R}^n 
$$

the gradient vector gives us the direction of steepest ascent for a single sample.
Since the gradient vector is the derivative of the $c_{nlm}^Z$ coefficients, 
directly interpreting the gradient is challenging. 
Due to the nature of the SNAP encoding, negative $c_{nlm}^Z$ coefficients do not necessarily correlate to negative densities and vice versa.
The gradient is therefore scaled back using the techniques introduced in \ref{ch:snap}.
This gives a 3d dimensional density space in which we can perform interpretations of the result. %TODO: Add figure
Here we encounter the encoding limitations introduced earlier.
While for some atoms, the encoding seems to work as expected, producing regions of negative density where we expect
an increase in the activation barrier, for others the distribution is not precise enough for detailed analysis.
In general, the detail of the encoding does not seem high enough for localized spacial interpretation of the gradient.
Possible solutions to the resolution problem would be to dramatically increase the number of coefficients used to describe a single sample.
This comes with it's own set of challenges, since the size of the features does not scale linear, but rather in $\mathcal{O}(n_{max} \cdot {l_{max}}^3)$.
The cubic scale of $l_{max}$ implies a dramatic increase in the size of the training data, increasing computation time for booth feature generation and training of the regression model.

Using a single SNAP encoding on spaces that need approximation with high density therefore might not be viable, and other options to 3D density encoding could be explored.
A possible solution could be to generate multiple SNAP features at fixed positions with space.
Combining the resolution of these features could allow for a higher accuracy of encoding.
However due to the low resolution and strong divergence from the original center, 
this solution might only make interpretation harder rather than offering additional insight.
\\

Other methods of encoding 3D spaces could include voxel representations.
Using spherical gaussians similar to SNAP to encode species and other information about the molecule are thinkable.
A possible problem with voxel representations would again be creating a model invariant to rotation.
While this sounded counter intuitive at first, SNAP has shown that rotational invariance can be achieved using data augmentations.
However as shown the changes in the generated SNAP features are relatively small even under large rotations.
Data augmentation on 3D voxel representations in convolutional neural networks seems to achieve high accuracies in 
other fields \cite{7353481}.
Voxel representations could therefore be a possible solution to further increasing interpretability of the model.


\section{Outlook}

The possibility of mapping both ways between the real world and SNAP features is what set the SNAP feature generator apart 
from many other chemical feature generators.
It allows for many different approaches to computational element discovery.
\\
Another area where SNAP features could be used is auto encoders or generative adversarial neural networks (GANs).
Both autoencoders and GANs are machine learning methods that can be used to produce never seen before data with similar properties
to the training data.
GANs and autoencoders have recently become very popular in image synthesis for their ability to generate synthetic images 
virtually indistinguishable from real pictures \cite{karras2019stylebased} .
Using these techniques, but applying them to SNAP features of catalysts, rather than to images, synthesis 
of catalyst molecules is thinkable.
The approach is again limited by the accuracy of mapping between chemical space and feature space.
Another factor to be considered is if SNAP features actually encode all relevant information needed.

While the accuracies achieved by the models trained on SNAP features indicate that 
the majority of information needed to predict the activation barrier is encoded into the SNAP features, this does
not imply it holds true for other properties of the molecule.
In the future, the SNAP features might be aggregated with other properties of the molecule.
Possible ways to add more information to the SNAP features is to encode the space surrounding the 
central atom in the same way it is encoded now, but instead of encoding density information encoding 
other molecular properties, such as electro negativity.

While convolution layers did not prove effective in for the current SNAP features,
when changing the feature space encoded by snap, and therefor the dimensionality of the input vector,
convolutions might become relevant again.

In conclusion the SNAP feature extractor shows that data augmentation is possible in chemical spaces, and therefore 
feature generators that allow for a bi-directional mapping between features and chemical space can achieve similar or higher
accuracies to state-of-the-art one-directional feature extractors.
This allows for the use of bi-directional feature encoders in computational exploration and generation of chemical structures.
%TODO: Transfer LEarn