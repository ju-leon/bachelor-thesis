{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some_file.py\n",
    "#import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "#sys.path.insert(1, '/Users/leon/Files/dscribe')\n",
    "import dscribe\n",
    "import dscribe.descriptors\n",
    "#sys.path.insert(1, '/Users/leon/Files/dscribe/decriptors')\n",
    "from dscribe.descriptors import SOAP\n",
    "#from dscribe.descriptors import ACSF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from matplotlib.pyplot import imshow\n",
    "import math\n",
    "import sklearn\n",
    "#import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import regularizers\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import SGD, Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "from collections import namedtuple\n",
    "from ase.io import read\n",
    "from ase.build import molecule\n",
    "from ase import Atoms, Atom\n",
    "from ase.visualize import view\n",
    "from ase.geometry.analysis import Analysis\n",
    "import plotly.graph_objects as go\n",
    "from ase.data import vdw_radii\n",
    "from ase.data.colors import cpk_colors, jmol_colors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from soap_generation.alignment import align_elements\n",
    "from soap_generation.augment import augment_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "barriers = dict()\n",
    "\n",
    "with open('../data/vaskas_features_properties_smiles_filenames.csv', 'r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        #images.append(row[0])\n",
    "        #elos.append(row[1])\n",
    "        barriers[row[93]] = float(row[91])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "elems = []\n",
    "for f in tqdm(os.listdir(\"../data/coordinates_molSimplify/\")):\n",
    "    if f.endswith(\".xyz\"):\n",
    "        if (f == \"ir_tbp_1_dft-pet3_1_dft-py_1_dft-hicn_1_fluoride_1_smi1_1_s_1.xyz\"):\n",
    "            print(\"1:\" + str(len(labels)))\n",
    "\n",
    "        if (f == \"ir_tbp_1_dft-pet3_1_dft-py_1_dft-hicn_1_dft-cn_1_smi1_1_s_1.xyz\"):\n",
    "            print(\"2:\" + str(len(labels)))\n",
    "        elems.append(read(\"../data/coordinates_molSimplify/\" + f))\n",
    "        labels.append(barriers[f[:-4]])\n",
    "\n",
    "labels = np.array(labels)\n",
    "number_samples = len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elems = align_elements(elems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augment_steps = 30\n",
    "elems, labels = augment_elements(elems, labels, steps=augment_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"As\",\"Br\",\"I\",\"Ir\"]\n",
    "\n",
    "rcut = 12.0\n",
    "nmax = 8\n",
    "lmax = 4\n",
    "\n",
    "# Setting up the SOAP descriptor\n",
    "soap = dscribe.descriptors.SOAP(\n",
    "    species=species,\n",
    "    periodic=False,\n",
    "    rcut=rcut,\n",
    "    nmax=nmax,\n",
    "    lmax=lmax,\n",
    "    rbf=\"gto\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from decoder.soap import SOAPDecoder\n",
    "\n",
    "decoder = SOAPDecoder(rcut=rcut, nmax=nmax, lmax=lmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create soap coefficients\n",
    "atom_index = [[0]] * len(elems)\n",
    "features_soap = soap.create_coeffs(elems, positions=atom_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale coefficents\n",
    "soapScaler = StandardScaler()\n",
    "soapScaler.fit(features_soap)\n",
    "features_soap = soapScaler.transform(features_soap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale labels\n",
    "labels = np.array(labels)\n",
    "barrierScaler = StandardScaler()\n",
    "barrierScaler.fit(labels.reshape(-1, 1))\n",
    "labels = barrierScaler.transform(labels.reshape(-1, 1))\n",
    "labels = np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_soap = features_soap.reshape(\n",
    "    number_samples, augment_steps, -1)\n",
    "\n",
    "labels = labels.reshape(\n",
    "    number_samples, augment_steps, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density(coefficents, element=None, bond_width=0, resolution=10, opacity=0.1, atoms=None, is_diff=False, scale_limit=0):\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    X, Y, Z = np.mgrid[-10:10:(resolution * 1j), -10:10:(resolution * 1j), -10:10:(resolution * 1j)]\n",
    "    values = np.zeros((resolution,resolution,resolution))\n",
    "    \n",
    "    indices = range(len(species))\n",
    "    if atoms is not None:\n",
    "        indices = [species.index(atoms)]\n",
    "    \n",
    "    for index in indices:\n",
    "        values += decoder.density(coefficents, index, X, Y, Z)\n",
    "\n",
    "    if scale_limit == 0:\n",
    "        scale_limit = np.amax(np.abs(values))\n",
    "    print(scale_limit)\n",
    "    if is_diff:\n",
    "        fig.add_trace(go.Volume(\n",
    "            x=X.flatten(),\n",
    "            y=Y.flatten(),\n",
    "            z=Z.flatten(),\n",
    "            value=values.flatten(),\n",
    "            isomin=-scale_limit,\n",
    "            isomax=scale_limit,\n",
    "            opacity=opacity, # needs to be small to see through all surfaces\n",
    "            opacityscale=[(0, 1), (0.5, 0), (1, 1)],\n",
    "            surface_count=30, # needs to be a large number for good volume rendering\n",
    "            colorscale=\"RdBu\"\n",
    "            #hoverinfo='skip',\n",
    "        ))\n",
    "    else:\n",
    "        fig.add_trace(go.Volume(\n",
    "            x=X.flatten(),\n",
    "            y=Y.flatten(),\n",
    "            z=Z.flatten(),\n",
    "            value=values.flatten(),\n",
    "            isomin=0,\n",
    "            isomax=scale_limit,\n",
    "            opacity=opacity, # needs to be small to see through all surfaces\n",
    "            opacityscale=[(0, 0), (0.1, 0), (1, 1)],\n",
    "            surface_count=30, # needs to be a large number for good volume rendering\n",
    "            #hoverinfo='skip',\n",
    "        ))\n",
    "\n",
    "    \n",
    "    if element is not None:\n",
    "        for position, atomic_number in zip(element.get_positions(), element.get_atomic_numbers()):\n",
    "            x1 = position[0]\n",
    "            y1 = position[1]\n",
    "            z1 = position[2]\n",
    "            size = np.nan_to_num(vdw_radii[atomic_number]) * 0.3\n",
    "\n",
    "            if size == 0:\n",
    "                size = 0.5\n",
    "\n",
    "            phi = np.linspace(0, 2*np.pi, 10)\n",
    "            theta = np.linspace(-np.pi/2, np.pi/2, 10)\n",
    "            phi, theta=np.meshgrid(phi, theta)\n",
    "\n",
    "            x = np.cos(theta) * np.sin(phi) * size + x1\n",
    "            y = np.cos(theta) * np.cos(phi) * size + y1\n",
    "            z = np.sin(theta) * size + z1    \n",
    "            color_string = \"rgb(\" + str(jmol_colors[atomic_number][0] * 230) + \",\" + str(jmol_colors[atomic_number][1] * 230) + \",\" + str(jmol_colors[atomic_number][2] * 230) + \")\"\n",
    "\n",
    "            fig.add_trace(go.Mesh3d({'x':x.flatten(), \n",
    "                                     'y':y.flatten(),\n",
    "                                     'z':z.flatten(),\n",
    "                                     'alphahull': 0, \n",
    "                                     'color': color_string,\n",
    "                                    }))\n",
    "\n",
    "        if bond_width > 0:\n",
    "            bonds = Analysis(element).all_bonds[0]\n",
    "            positions = element.get_positions()\n",
    "\n",
    "            for index, bond in zip(range(len(positions)), bonds):\n",
    "                x = []\n",
    "                y = []\n",
    "                z = []\n",
    "                for atom in bond:\n",
    "                    x.append(positions[index][0])\n",
    "                    x.append(positions[atom][0])\n",
    "                    x.append(positions[index][0])\n",
    "\n",
    "                    y.append(positions[index][1])\n",
    "                    y.append(positions[atom][1])\n",
    "                    y.append(positions[index][1])\n",
    "\n",
    "                    z.append(positions[index][2])\n",
    "                    z.append(positions[atom][2])\n",
    "                    z.append(positions[index][2])\n",
    "\n",
    "                fig.add_trace(go.Scatter3d(\n",
    "                    x = x,\n",
    "                    y = y,\n",
    "                    z = z,\n",
    "                    mode = \"lines\",\n",
    "                    line =\n",
    "                        go.Line(\n",
    "                            color = \"rgb(10, 10, 10)\",\n",
    "                            width = bond_width\n",
    "                        )\n",
    "                ))\n",
    "\n",
    "    fig.update_layout(\n",
    "        height=1000,\n",
    "        showlegend=False, \n",
    "        scene=dict(\n",
    "            xaxis=dict(showticklabels=False, visible=False),\n",
    "            yaxis=dict(showticklabels=False, visible=False),\n",
    "            zaxis=dict(showticklabels=False, visible=False),\n",
    "        ))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 807\n",
    "rotation = 0\n",
    "\n",
    "coefficents = soapScaler.inverse_transform(features_soap[sample][0].flatten())\n",
    "\n",
    "element = elems[sample * 20 + 10]\n",
    "print(element)\n",
    "plot_density(coefficents, element, bond_width=4, resolution=40, opacity=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainX, testX, trainY, testY) = train_test_split(\n",
    "        features_soap, labels, test_size=0.2, random_state=32)\n",
    "\n",
    "(testX, valX, testY, valY) = train_test_split(\n",
    "        testX, testY, test_size=0.5, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"/pfs/work7/workspace/scratch/utpqw-data-0/bachelor-thesis/bachelor-thesis/code/\"\n",
    "sample = \"9:3:0.2\"\n",
    "trainX = np.load(dir + \"features_train_\" + sample + \".npy\")\n",
    "trainY = np.load(dir + \"labels_train_\" + sample + \".npy\")\n",
    "\n",
    "valX = np.load(dir + \"features_val_\" + sample + \".npy\")\n",
    "valY = np.load(dir + \"labels_val_\" + sample + \".npy\")\n",
    "\n",
    "testX = np.load(dir + \"features_test_\" + sample + \".npy\")\n",
    "testY = np.load(dir + \"labels_test_\" + sample + \".npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = trainX.reshape(-1, 12, int(trainX.shape[2] / 12), 1)\n",
    "testX = testX.reshape(-1, 12, int(testX.shape[2] / 12), 1)\n",
    "valX = valX.reshape(-1, 12, int(valX.shape[2] / 12), 1)\n",
    "trainY = trainY.flatten()\n",
    "testY = testY.flatten()\n",
    "valY = valY.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reg_stats(y_true,y_pred,scaler=None):\n",
    "  y_true = np.array(y_true)\n",
    "  y_pred = np.array(y_pred)\n",
    "  if scaler:\n",
    "    y_true_unscaled = scaler.inverse_transform(y_true)\n",
    "    y_pred_unscaled = scaler.inverse_transform(y_pred)\n",
    "  r2 = sklearn.metrics.r2_score(y_true,y_pred)\n",
    "  mae = sklearn.metrics.mean_absolute_error(y_true_unscaled, y_pred_unscaled)\n",
    "  return r2,mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "  plt.plot(history.history['loss'], label='loss')\n",
    "  plt.plot(history.history['val_loss'], label='val_loss')\n",
    "  plt.ylim([0, 0.5])\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Loss')\n",
    "  plt.legend()\n",
    "  plt.grid(True)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(hp):\n",
    "    input_shape = trainX[0].shape\n",
    "    inputs = tf.keras.Input(shape=input_shape)\n",
    "\n",
    "    x = inputs\n",
    "    x = tf.keras.layers.Flatten()(x)\n",
    "\n",
    "    for i in range(hp.Int('hidden_layers', 1, 6, default=3)):\n",
    "        size = hp.Int('hidden_size_' + str(i), 10, 700, step=40)\n",
    "        reg = hp.Float('hidden_reg_' + str(i), 0,\n",
    "                       0.06, step=0.01, default=0.02)\n",
    "        dropout = hp.Float('hidden_dropout_' + str(i),\n",
    "                           0, 0.5, step=0.1, default=0.2)\n",
    "\n",
    "        x = tf.keras.layers.Dense(size, activation=\"relu\",\n",
    "                                  kernel_regularizer=regularizers.l2(reg))(x)\n",
    "        x = tf.keras.layers.Dropout(dropout)(x)\n",
    "\n",
    "        norm = hp.Choice('hidden_batch_norm_' + str(i), values=[True, False])\n",
    "\n",
    "        if norm:\n",
    "            x = tf.keras.layers.BatchNormalization()(x)\n",
    "\n",
    "    x = tf.keras.layers.Dense(1, kernel_regularizer='l2')(x)\n",
    "\n",
    "    model = tf.keras.Model(inputs=inputs, outputs=x)\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(\n",
    "            hp.Float('learning_rate', 1e-6, 1e-4, sampling='log')),\n",
    "        loss='mean_squared_error',\n",
    "        metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner.tuners import Hyperband\n",
    "import kerastuner as kt\n",
    "\n",
    "tuner = kt.Hyperband(\n",
    "    get_model,\n",
    "    objective='val_mean_squared_error',\n",
    "    max_epochs=1200,\n",
    "    project_name=\"Hyperband_FINAL_SNAP_9:3:0.4\",\n",
    "    directory=\"/pfs/work7/workspace/scratch/utpqw-data-0/bachelor-thesis/bachelor-thesis/code\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 0.2\n",
    "\n",
    "trainX = np.concatenate((trainX, valX))\n",
    "trainY = np.concatenate((trainY, valY))\n",
    "\n",
    "(trainX, valX, trainY, valY) = train_test_split(\n",
    "        trainX, trainY, test_size=split / 0.8, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner.results_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full = get_model(tuner.get_best_hyperparameters(4)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils.vis_utils import plot_model, model_to_dot\n",
    "import visualkeras\n",
    "\n",
    "model_full.summary()\n",
    "visualkeras.layered_view(model_full).show()\n",
    "#plot_model(model_full, show_shapes=True, show_layer_names=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = tf.keras.optimizers.Adam(learning_rate=tuner.get_best_hyperparameters(1)[0][\"learning_rate\"])\n",
    "\n",
    "model_full.compile(loss=\"mean_squared_error\", optimizer=opt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "H = model_full.fit(x=trainX, y=trainY, validation_data=(valX, valY), epochs=2000, batch_size=400, verbose=2, callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=80)])\n",
    "plot_loss(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r2_train, mae_train = reg_stats(trainY, model_full.predict(trainX), barrierScaler)\n",
    "r2_val, mae_val = reg_stats(valY, model_full.predict(valX), barrierScaler)\n",
    "r2_test, mae_test = reg_stats(testY, model_full.predict(testX), barrierScaler)\n",
    "print(\"R^2 Train: \" + str(r2_train))\n",
    "print(\"MAE Train: \" + str(mae_train))\n",
    "print(\"\")\n",
    "print(\"R^2 Validation: \" + str(r2_val))\n",
    "print(\"MAE Validation: \" + str(mae_val))\n",
    "print(\"\")\n",
    "print(\"R^2 Test: \" + str(r2_test))\n",
    "print(\"MAE Test: \" + str(mae_test))\n",
    "\n",
    "train_y_pred = barrierScaler.inverse_transform(model_full.predict(trainX))\n",
    "train_y_real = barrierScaler.inverse_transform(trainY)\n",
    "\n",
    "val_y_pred = barrierScaler.inverse_transform(model_full.predict(valX))\n",
    "val_y_real = barrierScaler.inverse_transform(valY)\n",
    "\n",
    "test_y_pred = barrierScaler.inverse_transform(model_full.predict(testX))\n",
    "test_y_real = barrierScaler.inverse_transform(testY)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(train_y_real, train_y_pred, marker=\"o\", c=\"C1\", label=\"Training\")\n",
    "ax.scatter(val_y_real, val_y_pred, marker=\"o\", c=\"C3\", label=\"Validation\")\n",
    "ax.scatter(test_y_real, test_y_pred, marker=\"o\", c=\"C2\", label=\"Testing\")\n",
    "ax.set_aspect('equal')\n",
    "ax.set_xlabel(\"Calculated barrier [kcal/mol]\")\n",
    "ax.set_ylabel(\"Predicted barrier [kcal/mol]\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.savefig(\"../figs/scatter_\" + str(nmax) + \"-\" + str(lmax) + \"-\" + str(split) + \".png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "file = open(\"../figs/out_hyperparam.csv\", \"a\")\n",
    "file.write(str(30))\n",
    "file.write(\",\")\n",
    "file.write(str(split))\n",
    "file.write(\",\")\n",
    "file.write(str(nmax))\n",
    "file.write(\",\")\n",
    "file.write(str(lmax))\n",
    "file.write(\",\")\n",
    "file.write(str(rcut))\n",
    "file.write(\",\")\n",
    "file.write(str(r2_test))\n",
    "file.write(\",\")\n",
    "file.write(str(mae_test))\n",
    "file.write(\"\\n\")\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model_full.save(\"conv_20:3:3.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full = keras.models.load_model(\"model_8-4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_rotations, labels_rotations = augment_elements([elems[665]], labels, steps=360)\n",
    "atom_index = [[0]] * len(element_rotations)\n",
    "features_rotations = soap.create_coeffs(element_rotations, positions=atom_index)\n",
    "\n",
    "features_rotations = soapScaler.transform(features_rotations)\n",
    "\n",
    "plt.imshow(features_rotations[0].reshape(12,200))\n",
    "y_axis = np.arange(0, 12, 1)\n",
    "plt.yticks(y_axis, species)\n",
    "plt.yticks(fontsize=7)\n",
    "plt.ylabel('$Z$')\n",
    "plt.xlabel('$c^Z_{nlm}$')\n",
    "plt.title('Rotation 0째')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(features_rotations[5].reshape(12,200))\n",
    "y_axis = np.arange(0, 12, 1)\n",
    "plt.yticks(y_axis, species)\n",
    "plt.yticks(fontsize=7)\n",
    "plt.ylabel('$Z$')\n",
    "plt.xlabel('$c^Z_{nlm}$')\n",
    "plt.title('Rotation 90째')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(features_rotations[10].reshape(12,200))\n",
    "y_axis = np.arange(0, 12, 1)\n",
    "plt.yticks(y_axis, species)\n",
    "plt.yticks(fontsize=7)\n",
    "plt.xlabel('$c^Z_{nlm}$')\n",
    "plt.ylabel('$Z$')\n",
    "plt.title('Rotation 180째')\n",
    "plt.show()\n",
    "\n",
    "plt.imshow(features_rotations[15].reshape(12,200))\n",
    "y_axis = np.arange(0, 12, 1)\n",
    "plt.yticks(y_axis, species)\n",
    "plt.yticks(fontsize=7)\n",
    "plt.ylabel('$Z$')\n",
    "plt.xlabel('$c^Z_{nlm}$')\n",
    "plt.title('Rotation 270째')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model_full = keras.models.load_model(\"/Users/leon/out/model____augment_steps=30_l=4_n=8_split=0.3_rcut=12.h5\")\n",
    "\n",
    "#scaler = copy.deepcopy(soapScaler)\n",
    "\n",
    "nmax = 8\n",
    "lmax = 4\n",
    "\n",
    "# Setting up the SOAP descriptor\n",
    "soap = dscribe.descriptors.SOAP(\n",
    "    species=species,\n",
    "    periodic=False,\n",
    "    rcut=rcut,\n",
    "    nmax=nmax,\n",
    "    lmax=lmax,\n",
    "    rbf=\"gto\"\n",
    ")\n",
    "\n",
    "samples = [0, 74, 8, 898]\n",
    "colors = [\"C1\", \"C2\", \"C3\", \"C4\"]\n",
    "\n",
    "for sample, color in zip(samples, colors):\n",
    "    rotated, _ = augment_elements([elems[sample * augment_steps]], [0] * 3000, steps=360)\n",
    "    print(len(rotated))\n",
    "    \n",
    "    atom_index = [[0]] * len(rotated)\n",
    "    \n",
    "    species = [\"H\",\"C\",\"N\",\"O\",\"F\",\"P\",\"S\",\"Cl\",\"As\",\"Br\",\"I\",\"Ir\"]\n",
    "\n",
    "    rcut = 12.0\n",
    "\n",
    "    feature_rotations = soap.create_coeffs(rotated, positions=atom_index)\n",
    "    feature_rotations = soapScaler.transform(feature_rotations)\n",
    "    \n",
    "    feature_rotations = feature_rotations.reshape(-1, 12, int(feature_rotations.shape[1] / 12), 1)\n",
    "    \n",
    "    print(feature_rotations.shape)\n",
    "    predictions = model_full.predict(feature_rotations)\n",
    "\n",
    "    predictions = barrierScaler.inverse_transform(predictions)\n",
    "   \n",
    "    plt.step(np.arange(0,360), predictions, color=color, label=elems[sample].symbols)\n",
    "    \n",
    "    label = [labels[sample][0]] * 360\n",
    "    plt.step(np.arange(0,360), barrierScaler.inverse_transform(label), \":\", color=color)\n",
    "\n",
    "    #plt.step(np.arange(0,360), [barrierScaler.inverse_transform(labels[sample][0])] * 360, label='real')\n",
    "\n",
    "steps = np.linspace(0, 360, 31)\n",
    "\n",
    "for step in steps:\n",
    "    plt.vlines(x=step, ymin=0, ymax=100, colors=\"k\", linestyle=\":\", linewidth=0.6)\n",
    "    \n",
    "plt.grid(axis='x', color='0.95')\n",
    "plt.xlabel('Rotation [degrees]')\n",
    "plt.ylabel('Prediction [kcal/mol]')\n",
    "plt.ylim([8,19.5])\n",
    "\n",
    "steps = np.linspace(0, 360, 11)\n",
    "plt.xticks(steps)\n",
    "#plt.legend()\n",
    "#plt.title('Prediction over rotation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "view(rotated[60], viewer='x3d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model_full.layers:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.explainers._deep.deep_tf.op_handlers[\"AddV2\"] = shap.explainers._deep.deep_tf.passthrough\n",
    "\n",
    "# select a set of background examples to take an expectation over\n",
    "background = trainX[np.random.choice(trainX.shape[0], 100, replace=False)]\n",
    "print(background.shape)\n",
    "# explain predictions of the model on four images\n",
    "e = shap.DeepExplainer(model_full, background)\n",
    "# ...or pass tensors directly\n",
    "# e = shap.DeepExplainer((model.layers[0].input, model.layers[-1].output), background)\n",
    "\n",
    "features_soap = features_soap.reshape(-1, 12, int(features_soap.shape[2] / 12), 1)\n",
    "selection = [0,20,40,60]\n",
    "shap_values = e.shap_values(features_soap[selection])\n",
    "#print(shap_values.shape)\n",
    "# plot the feature attributions\n",
    "shap.image_plot(shap_values, -features_soap[selection])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.sum(shap_values[0][0]).reshape(-1,1))\n",
    "barrierScaler.inverse_transform(np.sum(shap_values[0][0]).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(shap_values).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(features_soap[selection[0]].reshape(12,48))\n",
    "y_axis = np.arange(0, 12, 1)\n",
    "plt.yticks(y_axis, species)\n",
    "plt.colorbar()\n",
    "plt.title(\"Features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.matshow(shap_values[0][0].reshape(12,48))\n",
    "y_axis = np.arange(0, 12, 1)\n",
    "plt.yticks(y_axis, species)\n",
    "plt.colorbar()\n",
    "plt.title(\"SHAP Values\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_scaled = barrierScaler.inverse_transform(shap_values[0][0].flatten())\n",
    "element = elems[selection[0]]\n",
    "imshow(shap_scaled.reshape(12,48))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_density_diff(shap, coefficents, scaler, element=None, bond_width=0, resolution=10, opacity=0.1, atoms=None, scale_limit=0):  \n",
    "    sc_scaled = scaler.inverse_transform(coefficents + shap)\n",
    "    c_scaled = scaler.inverse_transform(coefficents)\n",
    "    coeffs = sc_scaled - c_scaled\n",
    "    \n",
    "    plot_density(coeffs, element, bond_width, resolution, opacity, atoms, is_diff=True, scale_limit=scale_limit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_density(\n",
    "    shap_scaled, \n",
    "    element, \n",
    "    bond_width=4, \n",
    "    opacity=0.8, \n",
    "    resolution=20, \n",
    "    atoms=\"As\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "print(element)\n",
    "\n",
    "print(features_soap[selection[0]].shape)\n",
    "plot_density_diff(\n",
    "    shap_values[0][0].flatten(), \n",
    "    features_soap[selection[0]].flatten(), \n",
    "    soapScaler, \n",
    "    element, \n",
    "    bond_width=4, \n",
    "    opacity=0.3, \n",
    "    resolution=40, \n",
    "    atoms=\"As\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_contrib = np.sum(shap_values[0][0], axis = 1).flatten()\n",
    "plt.bar(list(range(len(species))), element_contrib)\n",
    "plt.ylabel(\"Influence on prediction\")\n",
    "plt.xlabel(\"Elements\")\n",
    "plt.xticks(list(range(len(species))), species)\n",
    "plt.title(\"Influence of features for species\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_full = keras.models.load_model(\"model_8-4.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 979#357 #807#979\n",
    "print(barrierScaler.inverse_transform(labels[807])[0])\n",
    "element = elems[index * 30 + 15]\n",
    "\n",
    "print(features_soap[index].shape)\n",
    "print(element)\n",
    "\n",
    "image = features_soap[index][0].reshape((12, int(features_soap[index].shape[1] / 12)))\n",
    "\n",
    "plt.imshow(image)\n",
    "y_axis = np.arange(0, 12, 1)\n",
    "plt.yticks(y_axis, species)\n",
    "plt.yticks(fontsize=7)\n",
    "plt.show()\n",
    "\n",
    "#explainer.save(grid, '.', 'act.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_scaled = barrierScaler.transform(np.array([[0]]))[0]\n",
    "print(zero_scaled)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    inputs = tf.cast(image.reshape((1, 12, int(features_soap[index].shape[1] / 12), 1)), tf.float32)\n",
    "    tape.watch(inputs)\n",
    "    prediction = model_full(inputs)\n",
    "\n",
    "    print(\"Prediction: \" + str(prediction))\n",
    "\n",
    "grid = tape.gradient(prediction, inputs).numpy()\n",
    "print(grid.shape)\n",
    "\n",
    "plt.imshow(grid.reshape(12,int(features_soap[index].shape[1] / 12)))\n",
    "y_axis = np.arange(0, 12, 1)\n",
    "plt.yticks(y_axis, species)\n",
    "plt.yticks(fontsize=7)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_gradient(model, sample, epsilon=1):\n",
    "    gradient = np.zeros(sample.flatten().shape)\n",
    "    \n",
    "    # Scedule all gradients to be computed\n",
    "    gradients = []\n",
    "    for index in range(len(gradient)):\n",
    "        direction = np.zeros(sample.flatten().shape)\n",
    "        direction[index] = epsilon\n",
    "        \n",
    "        gradients += [(sample.flatten() + direction).reshape(sample.shape)]\n",
    "    \n",
    "    \n",
    "    gradients = np.array(gradients)\n",
    "    new_shape = [-1] + list(sample.shape)\n",
    "\n",
    "    predictions = model.predict(gradients.reshape(new_shape))\n",
    "    \n",
    "    value = model.predict(np.array([sample]))\n",
    "    \n",
    "    gradients = (predictions - value) / epsilon\n",
    "    \n",
    "    return gradients.reshape(sample.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = compute_gradient(model_full, image.reshape(12,200), epsilon=3).reshape(1,12,200,1)\n",
    "\n",
    "plt.imshow(grid.reshape(12,200))\n",
    "y_axis = np.arange(0, 12, 1)\n",
    "plt.yticks(y_axis, species)\n",
    "plt.yticks(fontsize=7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(element)\n",
    "plot_density_diff(grid.flatten(), image.flatten(), soapScaler, element, bond_width=4, opacity=1, resolution=40, atoms=\"N\")\n",
    "#0.0013641491280516759"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resolution = 20\n",
    "influence = []\n",
    "for atom in range(len(species)):\n",
    "    X, Y, Z = np.mgrid[-10:10:(resolution * 1j), -10:10:(resolution * 1j), -10:10:(resolution * 1j)]\n",
    "    values = np.zeros((resolution,resolution,resolution))\n",
    "\n",
    "    indices = range(len(species))\n",
    "\n",
    "    sc_scaled = soapScaler.inverse_transform(image.flatten() + grid.flatten())\n",
    "    c_scaled = soapScaler.inverse_transform(image.flatten())\n",
    "\n",
    "    values += decoder.density(sc_scaled, atom, X, Y, Z) - decoder.density(c_scaled, atom, X, Y, Z)\n",
    "\n",
    "    influence.append(np.sum(values))\n",
    "    \n",
    "\n",
    "\n",
    "plt.bar(list(range(len(species))), influence)\n",
    "plt.ylabel(\"Gradient density\")\n",
    "plt.xlabel(\"Elements\")\n",
    "plt.xticks(list(range(len(species))), species)\n",
    "plt.title(\"$\\sum \\rho_\\nabla^Z$gradient density space\")\n",
    "plt.ylim([-0.5,0.1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "zero_scaled = barrierScaler.transform(np.array([[0]]))[0]\n",
    "print(zero_scaled)\n",
    "\n",
    "influence_global = np.zeros(12)\n",
    "\n",
    "print(len(testX))\n",
    "for sample in testX:\n",
    "    print(sample.shape)\n",
    "    with tf.GradientTape() as tape:\n",
    "        inputs = tf.cast(sample[0].reshape((1, 12, int(sample.shape[1] / 12), 1)), tf.float32)\n",
    "        tape.watch(inputs)\n",
    "        prediction = model_full(inputs)\n",
    "\n",
    "        print(\"Prediction: \" + str(prediction))\n",
    "\n",
    "    grid = tape.gradient(prediction, inputs).numpy()\n",
    "\n",
    "    resolution = 10\n",
    "    influence = []\n",
    "    for atom in range(len(species)):\n",
    "        X, Y, Z = np.mgrid[-10:10:(resolution * 1j), -10:10:(resolution * 1j), -10:10:(resolution * 1j)]\n",
    "        values = np.zeros((resolution,resolution,resolution))\n",
    "\n",
    "        indices = range(len(species))\n",
    "\n",
    "        sc_scaled = soapScaler.inverse_transform(image.flatten() + grid.flatten())\n",
    "        c_scaled = soapScaler.inverse_transform(image.flatten())\n",
    "\n",
    "        values += decoder.density(sc_scaled, atom, X, Y, Z) - decoder.density(c_scaled, atom, X, Y, Z)\n",
    "\n",
    "        influence.append(np.sum(values))\n",
    "    \n",
    "    influence_global += np.array(influence)\n",
    "\n",
    "\n",
    "plt.bar(list(range(len(species))), influence_global / len(testX))\n",
    "plt.ylabel(\"Gradient density\")\n",
    "plt.xlabel(\"Elements\")\n",
    "plt.xticks(list(range(len(species))), species)\n",
    "plt.title(\"Integration of gradient density space\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "element1 = features_soap[123][0] #809\n",
    "element2 = features_soap[147][15] #979\n",
    "\n",
    "def interpolate(alpha):\n",
    "    return (element1 * alpha) + (element2 * (1 - alpha))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "for i in range(15):\n",
    "    for j in range(15):\n",
    "        element1 = features_soap[369][i] #809\n",
    "        element2 = features_soap[456][j] \n",
    "        \n",
    "        interpolations = []\n",
    "        for x in np.linspace(0,1,50):\n",
    "            interpolations.append(interpolate(x))\n",
    "\n",
    "        interpolations = np.array(interpolations).reshape(-1,12,200,1)\n",
    "        \n",
    "        predictions.append(barrierScaler.inverse_transform(model_full.predict(interpolations)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for prediction in predictions:\n",
    "    plt.plot(np.linspace(0,1,50), prediction, \"-\")\n",
    "\n",
    "plt.xlabel('Interpolation [$ x $]')\n",
    "plt.ylabel('MAE [$kcal/mol$]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
